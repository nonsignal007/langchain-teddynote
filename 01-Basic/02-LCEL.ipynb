{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "\n",
    "- 기본 예시 : 프롬프트 + 모델 + 출력 파서\n",
    "    - 가장 기본적이고 일반적인 사용 사례는 prompt 템플릿과 모델을 함께 연결하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['LANGSMITH_PROJECT'] = 'CH01-Basic'\n",
    "os.environ['HF_HOME'] = '../' # model weight 저장 위치를 현재 폴더로 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프롬프트 템플릿의 활용\n",
    "\n",
    "- `PromptTemplate`\n",
    "    - 사용자의 입력 변수를 사용하여 완전한 프롬프트 문자열을 만드는 데 사용되는 템플릿 입니다.\n",
    "\n",
    "- 사용법\n",
    "    - `template` : 템플릿 문자열이다. 이 문자열 내에서 중괄호 `{}` 는 변수를 나타낸다.\n",
    "    - `input_variables` : 중괄호 안에 들어갈 변수의 이름을 리스트로 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='{country}의 수도는 어디인가요?')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# template 정의\n",
    "template = \"{country}의 수도는 어디인가요?\"\n",
    "\n",
    "# PromptTmeplate 객체 생성\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국의 수도는 어디인가요?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 생성\n",
    "prompt = prompt_template.format(country=\"대한민국\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain 생성\n",
    "\n",
    "- LCEL 을 사용하여 다양한 구성 요소를 단일 체인으로 결합한다.\n",
    "- 서로 다른 구성 요소를 연결하고 한 구성 요소의 출력을 다음 구성 요소의 입력으로 전달한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"Please explain {topic} in simple terms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 설정\n",
    "\n",
    "- HuggingFace 내에 있는 `DeepSeek-R1-Distill-Qwen-1.5B` 모델을 사용하도록 하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def model_generate(prompt):\n",
    "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "    encode_token = tokenizer(prompt.text, return_tensors='pt', padding=True, truncation=True)\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids = encode_token['input_ids'].to(device),\n",
    "            attention_mask = encode_token['attention_mask'].to(device),\n",
    "            temperature = 0.6,\n",
    "            max_new_tokens = 50\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model_generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `invoke()` 호출\n",
    "\n",
    "- python 딕셔너리 형태로 입력값을 전달한다.\n",
    "- `invoke()` 함수 호출 시 입력값을 전달합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {'topic' : 'Principles of AI Model Training'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Please explain Principles of AI Model Training in simple terms. I need to understand this for my project. Please explain step by step.\\n\\nOkay, so I need to explain the principles of AI model training in simple terms. I know a little about AI, but I'm not an expert. Let me start by\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please explain Principles of AI Model Training in simple terms. Need to explain in a way that someone with no technical background can understand. Also, need to explain why each principle is important. Maybe some examples would help.\n",
      "\n",
      "Also, I have to make sure that the explanation is clear and concise, avoiding jargon"
     ]
    }
   ],
   "source": [
    "answer = chain.stream(input)\n",
    "\n",
    "for chunk in answer:\n",
    "    print(chunk, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 출력 파서 (Output Parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model_generate | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Please explain Principle of AI model Training in simple terms. I need to understand it for my AI project.\\nOkay, so I have a project where I'm trying to build an AI model that can predict stock prices. I'm a bit confused about how AI models are trained. I remember from school that there\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {'topic' : 'Principle of AI model Training'}\n",
    "\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stream 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please explain Principle of AI model Training in simple terms. What are the key factors that determine the success of an AI model training?\n",
      "Please explain Principle of AI model Training in simple terms. What are the key factors that determine the success of an AI model training?\n",
      "</think>\n",
      "\n",
      "**AI Model Training Explained Simply"
     ]
    }
   ],
   "source": [
    "answer = chain.stream(input)\n",
    "\n",
    "for chunk in answer:\n",
    "    print(chunk , end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 템플릿을 변경하여 적용\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t•\t•\tQuestion:\n",
      "\t•\t•\tAnswer:\n",
      "\n",
      "Please ensure that your answers are in Korean and English.\n",
      "Alright, so I want to go to a restaurant and order food. First, I need to figure out the best time to get there. Maybe it's around lunchtime or early evening. I should probably book a table in advance because there might be competition. Once I have a seat, I can start ordering dishes. I'll make a list of what I want, like"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "template = \"\"\"\n",
    "You are a 10-year experienced Korean teacher. Please write an Korean conversation based on the given situation in the [FORMAT] below.\n",
    "\n",
    "Situation:\n",
    "{question}\n",
    "\n",
    "FORMAT:\n",
    "\t•\tKorean Conversation:\n",
    "\t•\tEnglish Translation:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "## model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "## output-parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "## Chain\n",
    "\n",
    "def model_generate(prompt):\n",
    "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "    encode_token = tokenizer(prompt.text, return_tensors='pt', padding=True, truncation=True)\n",
    "    model.to(device)\n",
    "\n",
    "    input_length = encode_token['input_ids'].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids = encode_token['input_ids'].to(device),\n",
    "            attention_mask = encode_token['attention_mask'].to(device),\n",
    "            temperature = 0.6,\n",
    "            max_new_tokens = 100\n",
    "        )\n",
    "    \n",
    "    generated_ids = output[0][input_length:]\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "chain = prompt | model_generate | output_parser\n",
    "\n",
    "answer = chain.stream({'question' : 'I want to go to a restaurant and order food.'})\n",
    "\n",
    "for chunk in answer:\n",
    "    print(chunk, end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
